{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Second Model v2\n",
    "\n",
    "The first model suffers from poor predictions in many cases, and particular issues when not drawing in the centers.\n",
    "\n",
    "This model aims to improve upon the first model in a few ways:\n",
    "* It uses more input transforms to create a translational invariance in the training data.\n",
    "* It better designs the convolution / pooling layers to better align for digit recognition, and translation equivariance/invariance\n"
   ],
   "id": "a4b737ef450b5d2"
  },
  {
   "cell_type": "code",
   "id": "e02b36b5-2127-4559-aa7f-a655a1dcdeab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T16:16:58.639808Z",
     "start_time": "2025-05-01T16:16:58.636745Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from importlib import resources\n",
    "from IPython.display import display\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "CURRENT_WEIGHTS_PATH = \"nn-invariant-v2-current.pth\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:55:29.043341Z",
     "start_time": "2025-05-01T15:55:29.014472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(dtype=torch.float32, scale=True),\n",
    "    v2.RandomResize(28, 40),\n",
    "    v2.RandomRotation(30),\n",
    "    v2.RandomResizedCrop(size = 28, scale = (28.0/40, 28.0/40)),\n",
    "])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(dtype=torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    \"./data\",\n",
    "    download=True,\n",
    "    transform=training_transform,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    \"./data\",\n",
    "    download=True,\n",
    "    transform=test_transform,\n",
    "    train=False,\n",
    ")\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")"
   ],
   "id": "ec7832a42f99d8fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 60000\n",
      "Test set size: 10000\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:55:31.113970Z",
     "start_time": "2025-05-01T15:55:31.096739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AssertShape(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: tuple | list[int],\n",
    "        label: str = None,\n",
    "        ignore_batch_size: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.size = tuple(size)\n",
    "        self.ignore_batch_size = ignore_batch_size\n",
    "        if label is not None:\n",
    "            self._label_prefix = f\"{label}: \"\n",
    "        else:\n",
    "            self._label_prefix = \"\"\n",
    "        self._cached_check_shape = torch.Size(size)\n",
    "        self._last_batch_size = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.ignore_batch_size:\n",
    "            if self._last_batch_size != x.shape[0]:\n",
    "                self._cached_check_shape = torch.Size(tuple([x.shape[0],] + [d for d in self.size]))\n",
    "            if x.shape != self._cached_check_shape:\n",
    "                raise ValueError(f\"{self._label_prefix}Expected tensor shape (*, {\", \".join(str(d) for d in self.size)}), got ({\", \".join(str(d) for d in x.shape)})\")\n",
    "        else:\n",
    "            if x.shape != self._cached_check_shape:\n",
    "                raise ValueError(f\"{self._label_prefix}Expected tensor shape ({\", \".join(str(d) for d in self.size)}), got ({\", \".join(str(d) for d in x.shape)})\")\n",
    "        return x\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Inspired by thinking in article:\n",
    "        # * https://chriswolfvision.medium.com/what-is-translation-equivariance-and-why-do-we-use-convolutions-to-get-it-6f18139d4c59\n",
    "        # A reproduction of:\n",
    "        # * https://www.kaggle.com/code/minggyul/digit-recognizer-using-cnn\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            AssertShape([1, 28, 28]),\n",
    "            nn.Conv2d(1, 32, kernel_size=3),  # AssertShape([32, 26, 26]),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),               # AssertShape([32, 26, 26]),\n",
    "            nn.Conv2d(32, 32, kernel_size=3), # AssertShape([32, 24, 24]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),               # AssertShape([32, 12, 12]),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3), # AssertShape([64, 10, 10]),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),               # AssertShape([64, 10, 10]),\n",
    "            nn.Conv2d(64, 64, kernel_size=3), # AssertShape([64, 8, 8]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),               # AssertShape([64, 4, 4]),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Flatten(),                     # AssertShape([64 * 4 * 4]),\n",
    "            nn.Linear(64 * 4 * 4, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(100, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "network = Network().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(network.parameters())"
   ],
   "id": "372b2e151158c359",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load weights\n",
    "state_dict = torch.load(resources.open_binary(__package__, CURRENT_WEIGHTS_PATH))\n",
    "network.load_state_dict(state_dict)"
   ],
   "id": "a0c0dde3b9c2af40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save weights as backup\n",
    "torch.save(network.state_dict(), \"../src/model/nn-invariant-backup-2-0.079.pth\")"
   ],
   "id": "e438f47524243476",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T16:05:16.217200Z",
     "start_time": "2025-05-01T15:55:40.318402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 20\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True, # Speed up CUDA\n",
    ")\n",
    "\n",
    "print_interval = 1000\n",
    "\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = network(inputs.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_interval == print_interval - 1:    # print every 1000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_interval:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    torch.save(network.state_dict(), CURRENT_WEIGHTS_PATH)\n",
    "    print(f'[{epoch + 1}, DONE!] model weights saved')\n",
    "\n",
    "print('Finished Training')"
   ],
   "id": "893ac589291218cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 0.426\n",
      "[1,  2000] loss: 0.223\n",
      "[1,  3000] loss: 0.166\n",
      "[1, DONE!] model weights saved\n",
      "[2,  1000] loss: 0.150\n",
      "[2,  2000] loss: 0.135\n",
      "[2,  3000] loss: 0.120\n",
      "[2, DONE!] model weights saved\n",
      "[3,  1000] loss: 0.112\n",
      "[3,  2000] loss: 0.106\n",
      "[3,  3000] loss: 0.102\n",
      "[3, DONE!] model weights saved\n",
      "[4,  1000] loss: 0.097\n",
      "[4,  2000] loss: 0.093\n",
      "[4,  3000] loss: 0.093\n",
      "[4, DONE!] model weights saved\n",
      "[5,  1000] loss: 0.093\n",
      "[5,  2000] loss: 0.087\n",
      "[5,  3000] loss: 0.083\n",
      "[5, DONE!] model weights saved\n",
      "[6,  1000] loss: 0.082\n",
      "[6,  2000] loss: 0.087\n",
      "[6,  3000] loss: 0.085\n",
      "[6, DONE!] model weights saved\n",
      "[7,  1000] loss: 0.076\n",
      "[7,  2000] loss: 0.077\n",
      "[7,  3000] loss: 0.077\n",
      "[7, DONE!] model weights saved\n",
      "[8,  1000] loss: 0.079\n",
      "[8,  2000] loss: 0.075\n",
      "[8,  3000] loss: 0.070\n",
      "[8, DONE!] model weights saved\n",
      "[9,  1000] loss: 0.070\n",
      "[9,  2000] loss: 0.072\n",
      "[9,  3000] loss: 0.069\n",
      "[9, DONE!] model weights saved\n",
      "[10,  1000] loss: 0.071\n",
      "[10,  2000] loss: 0.064\n",
      "[10,  3000] loss: 0.071\n",
      "[10, DONE!] model weights saved\n",
      "[11,  1000] loss: 0.070\n",
      "[11,  2000] loss: 0.065\n",
      "[11,  3000] loss: 0.065\n",
      "[11, DONE!] model weights saved\n",
      "[12,  1000] loss: 0.057\n",
      "[12,  2000] loss: 0.065\n",
      "[12,  3000] loss: 0.068\n",
      "[12, DONE!] model weights saved\n",
      "[13,  1000] loss: 0.058\n",
      "[13,  2000] loss: 0.059\n",
      "[13,  3000] loss: 0.062\n",
      "[13, DONE!] model weights saved\n",
      "[14,  1000] loss: 0.063\n",
      "[14,  2000] loss: 0.059\n",
      "[14,  3000] loss: 0.058\n",
      "[14, DONE!] model weights saved\n",
      "[15,  1000] loss: 0.057\n",
      "[15,  2000] loss: 0.061\n",
      "[15,  3000] loss: 0.061\n",
      "[15, DONE!] model weights saved\n",
      "[16,  1000] loss: 0.056\n",
      "[16,  2000] loss: 0.056\n",
      "[16,  3000] loss: 0.057\n",
      "[16, DONE!] model weights saved\n",
      "[17,  1000] loss: 0.057\n",
      "[17,  2000] loss: 0.054\n",
      "[17,  3000] loss: 0.054\n",
      "[17, DONE!] model weights saved\n",
      "[18,  1000] loss: 0.055\n",
      "[18,  2000] loss: 0.054\n",
      "[18,  3000] loss: 0.055\n",
      "[18, DONE!] model weights saved\n",
      "[19,  1000] loss: 0.054\n",
      "[19,  2000] loss: 0.056\n",
      "[19,  3000] loss: 0.051\n",
      "[19, DONE!] model weights saved\n",
      "[20,  1000] loss: 0.051\n",
      "[20,  2000] loss: 0.054\n",
      "[20,  3000] loss: 0.050\n",
      "[20, DONE!] model weights saved\n",
      "[21,  1000] loss: 0.053\n",
      "[21,  2000] loss: 0.054\n",
      "[21,  3000] loss: 0.050\n",
      "[21, DONE!] model weights saved\n",
      "[22,  1000] loss: 0.051\n",
      "[22,  2000] loss: 0.052\n",
      "[22,  3000] loss: 0.053\n",
      "[22, DONE!] model weights saved\n",
      "[23,  1000] loss: 0.054\n",
      "[23,  2000] loss: 0.047\n",
      "[23,  3000] loss: 0.053\n",
      "[23, DONE!] model weights saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 23\u001B[39m\n\u001B[32m     21\u001B[39m outputs = network(inputs.to(device))\n\u001B[32m     22\u001B[39m loss = criterion(outputs, labels.to(device))\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     24\u001B[39m optimizer.step()\n\u001B[32m     26\u001B[39m \u001B[38;5;66;03m# print statistics\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/NotWork/digit-recognizer-mli-project/.venv/lib/python3.12/site-packages/torch/_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/NotWork/digit-recognizer-mli-project/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/NotWork/digit-recognizer-mli-project/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    825\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    826\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T16:18:34.995174Z",
     "start_time": "2025-05-01T16:18:12.680818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    ")\n",
    "test_network = Network().to(device).eval()\n",
    "test_network.load_state_dict(torch.load(CURRENT_WEIGHTS_PATH))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "failed_ids = []\n",
    "for i, data in enumerate(test_loader):\n",
    "    inputs, labels = data\n",
    "    outputs = test_network(inputs.to(device))\n",
    "    # Batch size of 0\n",
    "    label = labels[0]\n",
    "    output = outputs[0]\n",
    "\n",
    "    total += 1\n",
    "    if np.argmax(F.softmax(output, dim=0).tolist()) == label:\n",
    "        correct += 1\n",
    "    else:\n",
    "        failed_ids.append(i)\n",
    "\n",
    "print(f\"Accuracy: {correct}/{total} ({(float(correct) / total):2.1%})\")"
   ],
   "id": "54330179fdeaa383",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9894/10000 (98.9%)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T16:18:40.294925Z",
     "start_time": "2025-05-01T16:18:40.159625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prints a random failed example\n",
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    failed_id = random.choice(failed_ids)\n",
    "    (example, label) = test_set[failed_id]\n",
    "    inputs = torch.stack([example])\n",
    "    temperature = 0.1\n",
    "    output = F.softmax(test_network(inputs.to(device)) * temperature, dim=1)[0]\n",
    "    labels = sorted(enumerate(output), key=lambda x: x[1], reverse=True)\n",
    "    display(v2.ToPILImage()(test_set.data[failed_id]))\n",
    "    print(f\"Expected: {label} --- Predicted: {\", \".join(f\"{label} ({prob:2.1%})\" for label, prob in labels[0:3])}\")\n"
   ],
   "id": "a6d01d5f10a832ca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxElEQVR4AWIgGzBCdK4QO37p1nkIG4Nk0lWe8GxVPDuGBBzIzbprBudgMNjyPiRhCCJA0F8nBAedxXpsgyi6GAIo/tNAcNAB64e96EJIoOY9Egcd8N4PgQsxwVlQhr7kAyiLAVPy8lNeuCQGsH2FEMIw1nkhQhIdsH6JRhdCgJwrPAgOGkj51YAkAhg0shkYOKYf/X7HM2hFG5IknJny79+/3/9e9ofq8cPF4DpZ/fN27FWVULO/yH/vrdnq41fgSoYKAwDg6TEPeHy62wAAAABJRU5ErkJggg==",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APBbSaK3uo5ZrWO6jU/NDKzBW+pUg/ka6DT9P0XxLcx2NmJNK1OUBIVml8y3nkJ4TOA0eeACSwz1I61ztxBLa3ElvPG0c0TFHRhgqQcEGo6KUEqQQcEcg11HxDlW48Xy3JCi4uLa2nugqhQJ3hRpOB0O4knk8k1y1Fd7pVr8OLWSwu7rV9TuZREHltJrHEQlA+67K2Sueflzx3rC15IdR1a+1KTxFYXc9xI0zFYrhS7E9AGj49snpXP0UUUV/9k="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 2 --- Predicted: 1 (16.6%), 2 (14.2%), 7 (10.9%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxElEQVR4AWIgGzBCdK4QO37p1nkIG4Nk0lWe8GxVPDuGBBzIzbprBudgMNjyPiRhCCJA0F8nBAedxXpsgyi6GAIo/tNAcNAB64e96EJIoOY9Egcd8N4PgQsxwVlQhr7kAyiLAVPy8lNeuCQGsH2FEMIw1nkhQhIdsH6JRhdCgJwrPAgOGkj51YAkAhg0shkYOKYf/X7HM2hFG5IknJny79+/3/9e9ofq8cPF4DpZ/fN27FWVULO/yH/vrdnq41fgSoYKAwDg6TEPeHy62wAAAABJRU5ErkJggg==",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APBbSaK3uo5ZrWO6jU/NDKzBW+pUg/ka6DT9P0XxLcx2NmJNK1OUBIVml8y3nkJ4TOA0eeACSwz1I61ztxBLa3ElvPG0c0TFHRhgqQcEGo6KUEqQQcEcg11HxDlW48Xy3JCi4uLa2nugqhQJ3hRpOB0O4knk8k1y1Fd7pVr8OLWSwu7rV9TuZREHltJrHEQlA+67K2Sueflzx3rC15IdR1a+1KTxFYXc9xI0zFYrhS7E9AGj49snpXP0UUUV/9k="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 2 --- Predicted: 1 (16.6%), 2 (14.2%), 7 (10.9%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA8UlEQVR4AWIYcCB77u9fSeyu4En5/PfPn0rskhF//iz/82cxVknXH9/CmTb/uYuQZEIwTVl2rpTjY5iDEEFIyscySEces0ZIMTCwwDn8YgxGixn/M/yGiyAzDJZeKNz15w8XshgKePhnMzOKADL4+8cKmYsC+P/fEEYRQAbdf/OQuShA/uVfXxQBJMDX8+ceLudk3Prz59NSL2zSgInU//jz58/fP3/2yyMZBmFyH/vz58+lxz///PnzPpsRIgYntf78+fNQisGi4eGfP39y4MIQxrI/f64ngphSqw/8uQBiIOFJfxoEoVx2RRgLKkBXCgDKy1beoeANMwAAAABJRU5ErkJggg==",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uy8FfD258Wj7Xc6hb6TpImW3+23IyHlbhUjXI3tkjuMVi+KvD8vhbxRqGiTSiZrSXYJdu3euAVbHOMgg4yfqax6K9m+Evi+C/uNE8J3ugWEttYzTXv26TJ8kKjsXZem7oAx4HHGa8z8XayviHxdq2rImyO6uXkjXuFz8uffGM1i1NaW0l7eQWsIBlmkWNATjLE4HP4177Efh18Jo5NDvZ7nUdT1C3kttQu7RgWhQ9VIDfJnPbLcc15j478LeHtHS31Pwx4httT026baIDIPtEBxnDLwSPfAweDXFUUUUV//Z"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 2 --- Predicted: 9 (15.7%), 2 (15.0%), 4 (11.7%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3UlEQVR4AWKgFljyXwbJKCYkNgMD65Gvf5AEUCUF/5x4gVNygsAyJDlUndwin+YhS6KA0FuCKHwUO2PfM6NIooAgVhQuqk6B3xBJbgiFjbSYuRibMBgYrvz3XR/MYmCBUHCS67foSg4LVEczMDCoK4PIjTKiDIYbQCwU7AzmTeZiYIAywXwIcMtkYDAUZWBnYBKACDAwILzyxIOboUOA4R9PtR1ckhHOqntwiPeFYpLm4jlwIcAQrn39Yc/+8/33H22EyyExet///XK/NxuhmgHZn+xP2DN3IKkeakwAC+0wGlFldDEAAAAASUVORK5CYII=",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uh8H6fZXWrteatAs2kWCefeozMu5NwXaCuDuJYY5HT8K59iCxIGBngelJRWjZvqO1tItXZf7ReINGJNokOTsDc4xlgefY8VoTaVoaeCxqEWqyPrSXaRS2hTCBGVjkHqcFeT0rnqK9I+1+Am8PWt9qWoXl7qkiw/a9Ptbfy8lE2jMrLxzyxU/NxxkZrg9TvIr6/kmt7SO0t84igTkRqOgJ6sfUnk1Toooor//2Q=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 6 --- Predicted: 3 (12.3%), 5 (12.1%), 2 (11.2%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA9ElEQVR4AWIYYMAav/AfGESzoLuEKf7mXxiYzMuIKq0GkwHT/CBJJhABwixBIBKO6+AsEMgBa3hWd+wHmPEXJAYDArf+/v37tVacgcH3ClgWJgEC+iCRoyAWQxyICdYJszMNLI5GwCSfgsTBBIgBxTDJ+SA+h709H4hmYGBYCqVBwOAY2KK/f88eiOoAMZVBohCg/xQkgIQXgk0EEwyREhBFcJIHzmKIfoWkCcI0gMv+gwggk40gScAgxoJYqPgCiIsRcyDBBd+PbwDRYByKMPBOr7qQEIpxjP6nwNKP29uR/MfAAI1xZv7891MZ/v8BGzSgBABUM6kXFDyigAAAAABJRU5ErkJggg==",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+vU/A/wAD9X8X6RDq9zqMGm2NwCYSYzLI4BIztyABkev4Ve8Wfs/atoWjSahpWqJq3kqzzQ/Z/JcIBklfnbd344PpmvHq0tA0u41rXrLT7a0mu3mlUGGADey5+bGeBxnk8Cvqr4q6N4o1Hwlbad4OEkTLJiZIJlhJhCEBASRxnHA9KPh3eazoPw3uLnxo91HPYPM0jXR3OIlAI5/i745Oa+Q69c+AmueGtE1/Um1ueC1vJYkWzubg7UUZbzF3HhScp19Dz6++6n4/8I6Qrm88Racrp96NLhXccZ+6pJ6e1eY+K/j54Y1DRNQ0yz0q+vBcwSQZnVY4zkYB6k45z0B4r50oooor/9k="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 2 --- Predicted: 0 (22.0%), 8 (15.3%), 2 (11.2%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABBUlEQVR4AWKgC1C8+/8CwiImBBPEilP49x9EY8N1v//+PY9NgoGBQfH+378vzEEsTKx49e/fvz8qMSVAIOwvCPxoUwRx0LD8BZDc379/29AkQOA4VO7vd0yT/b///fv3dejRv3//PgapRcEgjYnaDFqxf/9+D0CRYQBrXMLLwMCg8urv31Y0yea/fw+A5BgYfP7+fQWRhAdfBgPDxc9gMQgJZkKB0Lu/f9XAbI5tGDrj+MEyDAwM5u4wFgMLnMXAwMfAx84Wl87AwPAJIgoYTPL7f0aG5btNpcVBws/9QSQSfgILn7+/52ghiYOZBq/+gKX/3E8G89GInGV///6dnYQmOqi4AGrZeBJF6gtpAAAAAElFTkSuQmCC",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uv8Ah14EuvHfiNLJd8VhDh7u4UfcTPQHBG49s+57VQ8b6NYeHvGep6RpstxLaWkojR7j75O0bs8Dvnt0rn6K+jYtZ0/4G/D3TbYWQufEGqJ58qMSo3Y6scdFyFxxnnpzWL8SH0z4hfC628e2Vmttf2k4gu1B52k7dpPG7lkIPYHFeGVLazLb3cMzxiVY5Fcxt0YA5wfrX0Z4hT4ZfFC4tNe1DxYLAwWoia1aeOBwMlsYcZJBJHy5B4x78f4/+IPhmHwV/wAIH4Ot2l04FfNu3yAdr7ztzyxLAZY8enbHj1FFFFf/2Q=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 0 --- Predicted: 6 (21.5%), 0 (18.4%), 8 (11.3%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABBUlEQVR4AWKgC1C8+/8CwiImBBPEilP49x9EY8N1v//+PY9NgoGBQfH+378vzEEsTKx49e/fvz8qMSVAIOwvCPxoUwRx0LD8BZDc379/29AkQOA4VO7vd0yT/b///fv3dejRv3//PgapRcEgjYnaDFqxf/9+D0CRYQBrXMLLwMCg8urv31Y0yea/fw+A5BgYfP7+fQWRhAdfBgPDxc9gMQgJZkKB0Lu/f9XAbI5tGDrj+MEyDAwM5u4wFgMLnMXAwMfAx84Wl87AwPAJIgoYTPL7f0aG5btNpcVBws/9QSQSfgILn7+/52ghiYOZBq/+gKX/3E8G89GInGV///6dnYQmOqi4AGrZeBJF6gtpAAAAAElFTkSuQmCC",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uv8Ah14EuvHfiNLJd8VhDh7u4UfcTPQHBG49s+57VQ8b6NYeHvGep6RpstxLaWkojR7j75O0bs8Dvnt0rn6K+jYtZ0/4G/D3TbYWQufEGqJ58qMSo3Y6scdFyFxxnnpzWL8SH0z4hfC628e2Vmttf2k4gu1B52k7dpPG7lkIPYHFeGVLazLb3cMzxiVY5Fcxt0YA5wfrX0Z4hT4ZfFC4tNe1DxYLAwWoia1aeOBwMlsYcZJBJHy5B4x78f4/+IPhmHwV/wAIH4Ot2l04FfNu3yAdr7ztzyxLAZY8enbHj1FFFFf/2Q=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 0 --- Predicted: 6 (21.5%), 0 (18.4%), 8 (11.3%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAt0lEQVR4AWIY1KD6H27nqX5ehSrJhMTV4lqHxGNgQJHkZyh4/UgLVR4O1v/9+++vH5yLolPZkPGy/UskOQYWBCdK9prl9w8IPjJL+NFfQwaGa9iN1ZBecZHBXAVZPcIrl7d2/2NgYUaWRNj5CWwgI7IkQidYVPc/mIISaJImn05BJbBQ31D8CRiaTg4UDWiSjPgc9H8NeyhCM6pOeQaxRagiCJUMSf/+zuVE4qMwxf+dEEIRoAkHAANxJ+8GCpYXAAAAAElFTkSuQmCC",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+tO00K7vNA1HWleFLSwaKN97ENI8hwFQY5OASc44B9hWZRRXsnw88UQ+HdL8M6FocttNquuamsmou67vs8QfYI8f3ioJz2B9wa8k1EAandgAACZ8Af7xqtRXc/B6AT/FDSWaB5lhE8xVBk5WFypHvu2/jiuQ1G1uLLUZ7e6ZTcRuRLtYNhu4yOpB4PvVWip7O+u9OulurG6ntbhQQssEhRwCMHBHPIJH41ATk5NFf/9k="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 1 --- Predicted: 7 (18.6%), 1 (17.8%), 9 (11.7%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA4klEQVR4AWIYbIAR3UGJ3I83oovBQOLvv7tgbAYWOIuBgYHVxz6cieEqshAMcAed+fvv79+/82ACSDRP2N+/f////fv3zRQkUSgw+wuV/PvGHCqEsPP3K9ZvDDX/GdolBSWhkkhAUB3Mufb3bwCYwYCkk+H9e6gYbmrev38nccnO//P3oS5MEuEgBgYGc/V2cUaGT4aGDAwMixgY4GErZc4QrSUuwPgfpouZAe4g2w4LiOiTmxAaEcCASW39DgqBv7+Od6pB5JDIqyCpr1ePeyOJgZhgB53gZbi29O02EH8IYwCO1U8+kSrN/wAAAABJRU5ErkJggg==",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+rNjp95qd3HaWNtLc3EhAWOJSxNaHifwvqfhDVv7M1ZIkuvLWXbHIHAU5xyPpWNRXqngn4tr4P8Ow6Ro/hWCbUpHxJdGYlp2LcfKFyeOAAevPc1rfHloLiy8MXt7aw2niO4ti97AhyyLhcA+wbcBn3rxWur8CWvg6bU5pvGWo3FtawhWiggiZvPbJyCyglQMD0znqMV6FP8WvBfhlf+KI8HQrdjIF3dRhSuR1ByXPU8ZFc1efGvxFqDmS70vQJpdu0Sy2AdgPqxNeb0UUUV//2Q=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 2 --- Predicted: 7 (29.1%), 2 (21.5%), 3 (13.4%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA7ElEQVR4AWIYUoARt2vF7BGSHDxvUBQ6xH2B82WPfl25cmWUNUyg4XI3H4zN0PTnz58/f/98CICIXPk0CSHHcPCWHAPDis/XwHLct/9PBzOg4MCfIAYGBgMFEDfh3J9kARADBg782Qxl8sZ8emfOCeVAgOHlr/VglvS6PztlwCwkYtKfT/4MDAzul/+u5UEShoJZfy8G8E/5+3calI9Csc368/nynz/bpFFEYYBtxt8/n1JYYFxUWiDn75+zqEKAwXl2d//8vekF56IwXL7+/fs/G0UIzjG6/OfPzeko4QKXY4h5fq9OFsEdnCwAZAJR7hD701AAAAAASUVORK5CYII=",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+ggjqOtFa1n4Z1u/tlubbTZ5IX+6+MBvcZ6ismvdfE/ivWtNt9I1XQbSw1fwpf2iW1vZT2McgglAAMbbRuDZ5AJPJIrio/D+n+B0i1TxhAlzq0g8200IYAHXD3GPup0IQcnGDgZrkNa1y/wBe1SXUL6bdK/AVRtWNR0VQOAo7Cs6vbfCniDQfhb4Knuo/ECapruoQiRNNt5BJBC/G0tjowHU5B7Y7143f6heapfS3t/cy3N1Kd0ksrFmY/U1Woooor//Z"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 8 --- Predicted: 4 (21.6%), 9 (13.9%), 8 (11.7%)\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
