{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Second Model\n",
    "\n",
    "The first model suffers from poor predictions in many cases, and particular issues when not drawing in the centers.\n",
    "\n",
    "This model aims to improve upon the first model in a few ways:\n",
    "* It uses more input transforms to create a translational invariance in the training data.\n",
    "* It better designs the convolution / pooling layers to better align for digit recognition, and translation equivariance/invariance\n"
   ],
   "id": "a4b737ef450b5d2"
  },
  {
   "cell_type": "code",
   "id": "e02b36b5-2127-4559-aa7f-a655a1dcdeab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:33:15.421699Z",
     "start_time": "2025-05-01T15:33:15.419323Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from importlib import resources\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "CURRENT_WEIGHTS_PATH = \"nn-invariant-current.pth\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:47:20.048392Z",
     "start_time": "2025-05-01T15:47:20.012850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(dtype=torch.float32),\n",
    "    v2.RandomResize(28, 40),\n",
    "    v2.RandomRotation(30),\n",
    "    v2.RandomResizedCrop(size = 28, scale = (28.0/40, 28.0/40)),\n",
    "    # Not sure if this is needed...\n",
    "    v2.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(dtype=torch.float32),\n",
    "    v2.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    \"./data\",\n",
    "    download=True,\n",
    "    transform=training_transform,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    \"./data\",\n",
    "    download=True,\n",
    "    transform=test_transform,\n",
    "    train=False,\n",
    ")\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")"
   ],
   "id": "ec7832a42f99d8fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 60000\n",
      "Test set size: 10000\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:24:42.758444Z",
     "start_time": "2025-05-01T15:24:42.741146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AssertShape(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: tuple | list[int],\n",
    "        label: str = None,\n",
    "        ignore_batch_size: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.size = tuple(size)\n",
    "        self.ignore_batch_size = ignore_batch_size\n",
    "        if label is not None:\n",
    "            self._label_prefix = f\"{label}: \"\n",
    "        else:\n",
    "            self._label_prefix = \"\"\n",
    "        self._cached_check_shape = torch.Size(size)\n",
    "        self._last_batch_size = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.ignore_batch_size:\n",
    "            if self._last_batch_size != x.shape[0]:\n",
    "                self._cached_check_shape = torch.Size(tuple([x.shape[0],] + [d for d in self.size]))\n",
    "            if x.shape != self._cached_check_shape:\n",
    "                raise ValueError(f\"{self._label_prefix}Expected tensor shape (*, {\", \".join(str(d) for d in self.size)}), got ({\", \".join(str(d) for d in x.shape)})\")\n",
    "        else:\n",
    "            if x.shape != self._cached_check_shape:\n",
    "                raise ValueError(f\"{self._label_prefix}Expected tensor shape ({\", \".join(str(d) for d in self.size)}), got ({\", \".join(str(d) for d in x.shape)})\")\n",
    "        return x\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Inspired by thinking in article:\n",
    "        # * https://chriswolfvision.medium.com/what-is-translation-equivariance-and-why-do-we-use-convolutions-to-get-it-6f18139d4c59\n",
    "        # A reproduction of:\n",
    "        # * https://www.kaggle.com/code/minggyul/digit-recognizer-using-cnn\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            AssertShape([1, 28, 28]),\n",
    "            nn.Conv2d(1, 32, kernel_size=3),  AssertShape([32, 26, 26]),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),               AssertShape([32, 26, 26]),\n",
    "            nn.Conv2d(32, 32, kernel_size=3), AssertShape([32, 24, 24]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),               AssertShape([32, 12, 12]),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3), AssertShape([64, 10, 10]),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),               AssertShape([64, 10, 10]),\n",
    "            nn.Conv2d(64, 64, kernel_size=3), AssertShape([64, 8, 8]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),               AssertShape([64, 4, 4]),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Flatten(),                     AssertShape([64 * 4 * 4]),\n",
    "            nn.Linear(64 * 4 * 4, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(100, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "network = Network().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(network.parameters())"
   ],
   "id": "372b2e151158c359",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load weights\n",
    "state_dict = torch.load(resources.open_binary(__package__, CURRENT_WEIGHTS_PATH))\n",
    "network.load_state_dict(state_dict)"
   ],
   "id": "a0c0dde3b9c2af40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save weights as backup\n",
    "torch.save(network.state_dict(), \"../src/model/nn-invariant-backup-2-0.079.pth\")"
   ],
   "id": "e438f47524243476",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:44:26.027874Z",
     "start_time": "2025-05-01T15:36:14.145246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 20\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True, # Speed up CUDA\n",
    ")\n",
    "\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = network(inputs.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    torch.save(network.state_dict(), CURRENT_WEIGHTS_PATH)\n",
    "    print(f'[{epoch + 1}, DONE!] model weights saved')\n",
    "\n",
    "print('Finished Training')"
   ],
   "id": "893ac589291218cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 0.057\n",
      "[1,  2000] loss: 0.054\n",
      "[1,  3000] loss: 0.055\n",
      "[1, DONE!] model weights saved\n",
      "[2,  1000] loss: 0.057\n",
      "[2,  2000] loss: 0.054\n",
      "[2,  3000] loss: 0.059\n",
      "[2, DONE!] model weights saved\n",
      "[3,  1000] loss: 0.056\n",
      "[3,  2000] loss: 0.052\n",
      "[3,  3000] loss: 0.059\n",
      "[3, DONE!] model weights saved\n",
      "[4,  1000] loss: 0.055\n",
      "[4,  2000] loss: 0.054\n",
      "[4,  3000] loss: 0.053\n",
      "[4, DONE!] model weights saved\n",
      "[5,  1000] loss: 0.054\n",
      "[5,  2000] loss: 0.051\n",
      "[5,  3000] loss: 0.049\n",
      "[5, DONE!] model weights saved\n",
      "[6,  1000] loss: 0.049\n",
      "[6,  2000] loss: 0.055\n",
      "[6,  3000] loss: 0.051\n",
      "[6, DONE!] model weights saved\n",
      "[7,  1000] loss: 0.054\n",
      "[7,  2000] loss: 0.050\n",
      "[7,  3000] loss: 0.050\n",
      "[7, DONE!] model weights saved\n",
      "[8,  1000] loss: 0.050\n",
      "[8,  2000] loss: 0.051\n",
      "[8,  3000] loss: 0.049\n",
      "[8, DONE!] model weights saved\n",
      "[9,  1000] loss: 0.044\n",
      "[9,  2000] loss: 0.050\n",
      "[9,  3000] loss: 0.049\n",
      "[9, DONE!] model weights saved\n",
      "[10,  1000] loss: 0.050\n",
      "[10,  2000] loss: 0.043\n",
      "[10,  3000] loss: 0.053\n",
      "[10, DONE!] model weights saved\n",
      "[11,  1000] loss: 0.045\n",
      "[11,  2000] loss: 0.053\n",
      "[11,  3000] loss: 0.046\n",
      "[11, DONE!] model weights saved\n",
      "[12,  1000] loss: 0.048\n",
      "[12,  2000] loss: 0.047\n",
      "[12,  3000] loss: 0.049\n",
      "[12, DONE!] model weights saved\n",
      "[13,  1000] loss: 0.046\n",
      "[13,  2000] loss: 0.048\n",
      "[13,  3000] loss: 0.052\n",
      "[13, DONE!] model weights saved\n",
      "[14,  1000] loss: 0.047\n",
      "[14,  2000] loss: 0.049\n",
      "[14,  3000] loss: 0.045\n",
      "[14, DONE!] model weights saved\n",
      "[15,  1000] loss: 0.047\n",
      "[15,  2000] loss: 0.047\n",
      "[15,  3000] loss: 0.047\n",
      "[15, DONE!] model weights saved\n",
      "[16,  1000] loss: 0.048\n",
      "[16,  2000] loss: 0.045\n",
      "[16,  3000] loss: 0.051\n",
      "[16, DONE!] model weights saved\n",
      "[17,  1000] loss: 0.044\n",
      "[17,  2000] loss: 0.047\n",
      "[17,  3000] loss: 0.047\n",
      "[17, DONE!] model weights saved\n",
      "[18,  1000] loss: 0.044\n",
      "[18,  2000] loss: 0.050\n",
      "[18,  3000] loss: 0.045\n",
      "[18, DONE!] model weights saved\n",
      "[19,  1000] loss: 0.046\n",
      "[19,  2000] loss: 0.047\n",
      "[19,  3000] loss: 0.046\n",
      "[19, DONE!] model weights saved\n",
      "[20,  1000] loss: 0.045\n",
      "[20,  2000] loss: 0.047\n",
      "[20,  3000] loss: 0.044\n",
      "[20, DONE!] model weights saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[64]\u001B[39m\u001B[32m, line 21\u001B[39m\n\u001B[32m     18\u001B[39m optimizer.zero_grad()\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# forward + backward + optimize\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m outputs = \u001B[43mnetwork\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     22\u001B[39m loss = criterion(outputs, labels.to(device))\n\u001B[32m     23\u001B[39m loss.backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/NotWork/digit-recognizer-mli-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/NotWork/digit-recognizer-mli-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[56]\u001B[39m\u001B[32m, line 62\u001B[39m, in \u001B[36mNetwork.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     61\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m---> \u001B[39m\u001B[32m62\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlayers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/NotWork/digit-recognizer-mli-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/NotWork/digit-recognizer-mli-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/NotWork/digit-recognizer-mli-project/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    238\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[32m    239\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m240\u001B[39m         \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    241\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/NotWork/digit-recognizer-mli-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/NotWork/digit-recognizer-mli-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/NotWork/digit-recognizer-mli-project/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:47:48.437921Z",
     "start_time": "2025-05-01T15:47:25.407411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    ")\n",
    "test_network = Network().to(device).eval()\n",
    "test_network.load_state_dict(torch.load(CURRENT_WEIGHTS_PATH))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "failed_ids = []\n",
    "for i, data in enumerate(test_loader):\n",
    "    inputs, labels = data\n",
    "    outputs = network(inputs.to(device))\n",
    "    # Batch size of 0\n",
    "    label = labels[0]\n",
    "    output = outputs[0]\n",
    "\n",
    "    total += 1\n",
    "    if np.argmax(F.softmax(output, dim=0).tolist()) == label:\n",
    "        correct += 1\n",
    "    else:\n",
    "        failed_ids.append(i)\n",
    "\n",
    "print(f\"Accuracy: {correct}/{total} ({(float(correct) / total):2.1%})\")"
   ],
   "id": "54330179fdeaa383",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9699/10000 (97.0%)\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:08:28.036272Z",
     "start_time": "2025-05-01T15:08:27.885603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prints a random failed example\n",
    "import random\n",
    "\n",
    "failed_id = random.choice(failed_ids)\n",
    "(example, label) = test_set[failed_id]\n",
    "inputs = torch.stack([example])\n",
    "output = F.softmax(network(inputs.to(device)), dim=1)[0]\n",
    "labels = sorted(enumerate(output), key=lambda x: x[1], reverse=True)\n",
    "print(f\"Expected label: {label}\")\n",
    "print(f\"Top labels: {\", \".join(f\"{label} ({prob:2.1%})\" for label, prob in labels[0:3])}\")\n",
    "v2.ToPILImage()(test_set.data[failed_id]).show()\n"
   ],
   "id": "a6d01d5f10a832ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected label: 0\n",
      "Top labels: 6 (44.7%), 0 (26.0%), 8 (20.1%)\n"
     ]
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
